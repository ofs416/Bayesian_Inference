{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU3QIoW8i1hN"
      },
      "source": [
        "This is the auxiliary code for the 3F8 coursework. Some parts are missing and should be completed by the student. These are Marked with XXX\n",
        "\n",
        "**Note that your changes will not be saved unless you click the \"Copy to Drive\" button above!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LUd9O5Wqjjn"
      },
      "source": [
        "# First we download the data\n",
        "\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UJVoJqzHc7DZC8YDCN8e15LA6DvEnsuY' -O X.txt\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=15HONyJXWARLhNAtNQXFWAH8pvGPahh_U' -O y.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaHsJIOviyFq"
      },
      "source": [
        "# We load the data\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "X = np.loadtxt('X.txt')\n",
        "y = np.loadtxt('y.txt')\n",
        "\n",
        "# We randomly permute the data\n",
        "\n",
        "permutation = np.random.permutation(X.shape[ 0 ])\n",
        "X = X[ permutation, : ]\n",
        "y = y[ permutation ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2_O3851i8ZA"
      },
      "source": [
        "# We plot the data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "##\n",
        "# Function that plots the points in 2D together with their labels\n",
        "#\n",
        "# Inputs:\n",
        "#\n",
        "# X: 2d array with the input features\n",
        "# y: 1d array with the class labels (0 or 1)\n",
        "#\n",
        "# Output: 2D matrices with the x and y coordinates of the points shown in the plot\n",
        "#\n",
        "\n",
        "def plot_data_internal(X, y):\n",
        "    x_min, x_max = X[ :, 0 ].min() - .5, X[ :, 0 ].max() + .5\n",
        "    y_min, y_max = X[ :, 1 ].min() - .5, X[ :, 1 ].max() + .5\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
        "    plt.figure()\n",
        "    plt.xlim(xx.min(None), xx.max(None))\n",
        "    plt.ylim(yy.min(None), yy.max(None))\n",
        "    ax = plt.gca()\n",
        "    ax.plot(X[y == 0, 0], X[y == 0, 1], 'ro', label = 'Class 1')\n",
        "    ax.plot(X[y == 1, 0], X[y == 1, 1], 'bo', label = 'Class 2')\n",
        "    plt.xlabel('X1')\n",
        "    plt.ylabel('X2')\n",
        "    plt.title('Plot data')\n",
        "    plt.legend(loc = 'upper left', scatterpoints = 1, numpoints = 1)\n",
        "    return xx, yy\n",
        "\n",
        "##\n",
        "# Function that plots the data without returning anything by calling \"plot_data_internal\".\n",
        "#\n",
        "# Input:\n",
        "#\n",
        "# X: 2d array with the input features\n",
        "# y: 1d array with the class labels (0 or 1)\n",
        "#\n",
        "# Output: Nothing.\n",
        "#\n",
        "\n",
        "def plot_data(X, y):\n",
        "    xx, yy = plot_data_internal(X, y)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtI5ODq3jxPT"
      },
      "source": [
        "plot_data(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyVkK6epj0hY"
      },
      "source": [
        "# We split the data into train and test sets\n",
        "\n",
        "n_train = 800\n",
        "X_train = X[ 0 : n_train, : ]\n",
        "X_test = X[ n_train :, : ]\n",
        "y_train = y[ 0 : n_train ]\n",
        "y_test = y[ n_train : ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e99TSgEVj4Ix"
      },
      "source": [
        "# The logistic function\n",
        "\n",
        "def logistic(x): return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "##\n",
        "# Function that makes predictions with a logistic classifier\n",
        "#\n",
        "# Input:\n",
        "#\n",
        "# X_tile: matrix of input features (with a constant 1 appended to the left) \n",
        "#         for which to make predictions\n",
        "# w: vector of model parameters\n",
        "#\n",
        "# Output: The predictions of the logistic classifier\n",
        "#\n",
        "\n",
        "def predict(X_tilde, w): return logistic(np.dot(X_tilde, w))\n",
        "\n",
        "##\n",
        "# Function that computes the average loglikelihood of the logistic classifier on some data.\n",
        "#\n",
        "# Input:\n",
        "#\n",
        "# X_tile: matrix of input features (with a constant 1 appended to the left) \n",
        "#         for which to make predictions\n",
        "# y: vector of binary output labels \n",
        "# w: vector of model parameters\n",
        "#\n",
        "# Output: The average loglikelihood\n",
        "#\n",
        "\n",
        "def compute_average_ll(X_tilde, y, w):\n",
        "    output_prob = predict(X_tilde, w)\n",
        "    return np.mean(y * np.log(output_prob) + (1 - y) * np.log(1.0 - output_prob))\n",
        "\n",
        "##\n",
        "# Function that expands a matrix of input features by adding a column equal to 1.\n",
        "#\n",
        "# Input:\n",
        "#\n",
        "# X: matrix of input features.\n",
        "#\n",
        "# Output: Matrix x_tilde with one additional constant column equal to 1 added.\n",
        "#\n",
        "\n",
        "def get_x_tilde(X): return np.concatenate((np.ones((X.shape[ 0 ], 1 )), X), 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGPVminrkBOj"
      },
      "source": [
        "##\n",
        "# Function that finds the model parameters by optimising the likelihood using gradient descent\n",
        "#\n",
        "# Input:\n",
        "#\n",
        "# X_tile_train: matrix of training input features (with a constant 1 appended to the left) \n",
        "# y_train: vector of training binary output labels \n",
        "# X_tile_test: matrix of test input features (with a constant 1 appended to the left) \n",
        "# y_test: vector of test binary output labels \n",
        "# alpha: step_size_parameter for the gradient based optimisation\n",
        "# n_steps: the number of steps of gradient based optimisation\n",
        "#\n",
        "# Output: \n",
        "# \n",
        "# 1 - Vector of model parameters w \n",
        "# 2 - Vector with average log-likelihood values obtained on the training set\n",
        "# 3 - Vector with average log-likelihood values obtained on the test set\n",
        "#\n",
        "\n",
        "def fit_w(X_tilde_train, y_train, X_tilde_test, y_test, n_steps, alpha):\n",
        "    w = np.random.randn(X_tilde_train.shape[ 1 ])\n",
        "    ll_train = np.zeros(n_steps)\n",
        "    ll_test = np.zeros(n_steps)\n",
        "    for i in range(n_steps):\n",
        "        sigmoid_value = predict(X_tilde_train, w)\n",
        "\n",
        "        w = # XXX Gradient-based update rule for w. To be completed by the student\n",
        "\n",
        "        ll_train[ i ] = compute_average_ll(X_tilde_train, y_train, w)\n",
        "        ll_test[ i ] = compute_average_ll(X_tilde_test, y_test, w)\n",
        "        print(ll_train[ i ], ll_test[ i ])\n",
        "\n",
        "    return w, ll_train, ll_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4NIZwbSkF-2"
      },
      "source": [
        "# We train the classifier\n",
        "\n",
        "alpha = # XXX Learning rate for gradient-based optimisation. To be completed by the student\n",
        "n_steps = # XXX Number of steps of gradient-based optimisation. To be completed by the student\n",
        "\n",
        "X_tilde_train = get_x_tilde(X_train)\n",
        "X_tilde_test = get_x_tilde(X_test)\n",
        "w, ll_train, ll_test = fit_w(X_tilde_train, y_train, X_tilde_test, y_test, n_steps, alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIInc5nqkNN_"
      },
      "source": [
        "##\n",
        "# Function that plots the average log-likelihood returned by \"fit_w\"\n",
        "#\n",
        "# Input:\n",
        "#\n",
        "# ll: vector with log-likelihood values\n",
        "#\n",
        "# Output: Nothing\n",
        "#\n",
        "\n",
        "def plot_ll(ll):\n",
        "    plt.figure()\n",
        "    ax = plt.gca()\n",
        "    plt.xlim(0, len(ll) + 2)\n",
        "    plt.ylim(min(ll) - 0.1, max(ll) + 0.1)\n",
        "    ax.plot(np.arange(1, len(ll) + 1), ll, 'r-')\n",
        "    plt.xlabel('Steps')\n",
        "    plt.ylabel('Average log-likelihood')\n",
        "    plt.title('Plot Average Log-likelihood Curve')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9WZfhQZkQyB"
      },
      "source": [
        "# We plot the training and test log likelihoods\n",
        "\n",
        "plot_ll(ll_train)\n",
        "plot_ll(ll_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeNgifPEkTjv"
      },
      "source": [
        "##\n",
        "# Function that plots the predictive probabilities of the logistic classifier\n",
        "#\n",
        "# Input:\n",
        "#\n",
        "# X: 2d array with the input features for the data (without adding a constant column with ones at the beginning)\n",
        "# y: 1d array with the class labels (0 or 1) for the data\n",
        "# w: parameter vector\n",
        "# map_inputs: function that expands the original 2D inputs using basis functions.\n",
        "#\n",
        "# Output: Nothing.\n",
        "#\n",
        "\n",
        "def plot_predictive_distribution(X, y, w, map_inputs = lambda x : x):\n",
        "    xx, yy = plot_data_internal(X, y)\n",
        "    ax = plt.gca()\n",
        "    X_tilde = get_x_tilde(map_inputs(np.concatenate((xx.ravel().reshape((-1, 1)), yy.ravel().reshape((-1, 1))), 1)))\n",
        "    Z = predict(X_tilde, w)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    cs2 = ax.contour(xx, yy, Z, cmap = 'RdBu', linewidths = 2)\n",
        "    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize = 14)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSeWaE6fkW2X"
      },
      "source": [
        "# We plot the predictive distribution\n",
        "\n",
        "plot_predictive_distribution(X, y, w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W_ILVQ-kcSo"
      },
      "source": [
        "##\n",
        "# Function that replaces initial input features by evaluating Gaussian basis functions\n",
        "# on a grid of points\n",
        "#\n",
        "# Inputs:\n",
        "#\n",
        "# l: hyper-parameter for the width of the Gaussian basis functions\n",
        "# Z: location of the Gaussian basis functions\n",
        "# X: points at which to evaluate the basis functions\n",
        "#\n",
        "# Output: Feature matrix with the evaluations of the Gaussian basis functions.\n",
        "#\n",
        "\n",
        "def evaluate_basis_functions(l, X, Z):\n",
        "    X2 = np.sum(X**2, 1)\n",
        "    Z2 = np.sum(Z**2, 1)\n",
        "    ones_Z = np.ones(Z.shape[ 0 ])\n",
        "    ones_X = np.ones(X.shape[ 0 ])\n",
        "    r2 = np.outer(X2, ones_Z) - 2 * np.dot(X, Z.T) + np.outer(ones_X, Z2)\n",
        "    return np.exp(-0.5 / l**2 * r2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvrU2_JckgFj"
      },
      "source": [
        "# We expand the data\n",
        "\n",
        "l = # XXX Width of the Gaussian basis funcction. To be completed by the student\n",
        "\n",
        "X_tilde_train = get_x_tilde(evaluate_basis_functions(l, X_train, X_train))\n",
        "X_tilde_test = get_x_tilde(evaluate_basis_functions(l, X_test, X_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgdhvtWLkjWd"
      },
      "source": [
        "# We train the new classifier on the feature expanded inputs\n",
        "\n",
        "alpha = # XXX Learning rate for gradient-based optimisation with basis functions. To be completed by the student\n",
        "n_steps = # XXX Number of steps of gradient-based optimisation with basis functions. To be completed by the student\n",
        "\n",
        "w, ll_train, ll_test = fit_w(X_tilde_train, y_train, X_tilde_test, y_test, n_steps, alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ-7dmLGkn6C"
      },
      "source": [
        "# We plot the training and test log likelihoods\n",
        "\n",
        "plot_ll(ll_train)\n",
        "plot_ll(ll_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxNi8HlTkr-8"
      },
      "source": [
        "# We plot the predictive distribution\n",
        "\n",
        "plot_predictive_distribution(X, y, w, lambda x : evaluate_basis_functions(l, x, X_train))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}